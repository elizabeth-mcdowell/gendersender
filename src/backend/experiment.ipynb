{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml \n",
    "from yaml import SafeLoader\n",
    "import config\n",
    "import openai\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = config.api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatModel:\n",
    "\n",
    "    def __init__(self, system: str, dir: str, temperature=0.3):\n",
    "\n",
    "        \"\"\"Sets up the model with a system prompt and a temperature\"\"\"\n",
    "        self.model = openai.ChatCompletion()\n",
    "        self.system = system\n",
    "        self.temperature = temperature\n",
    "        self.dir = dir\n",
    "        self.messages = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.messages = [{\"role\": \"system\", \"content\": self.system}]\n",
    "    \n",
    "    def few_shot(self, directify: bool):\n",
    "\n",
    "        \"\"\"\n",
    "        Loads the examples from the given directory and adds them to the messages depending on the direction of translation\n",
    "        dir: directory of the examples\n",
    "        directify: when set to true, sets up the model to translate from polite->direct\n",
    "        \"\"\"\n",
    "\n",
    "        if directify:\n",
    "            before, after = \"polite\", \"direct\"\n",
    "        else:\n",
    "            before, after = \"direct\", \"polite\"\n",
    "\n",
    "        path = os.path.join(os.getcwd(), self.dir)\n",
    "        for file in os.listdir(path):\n",
    "            with open(f\"{path}/{file}\") as f:\n",
    "                data = yaml.load(f, Loader=SafeLoader)\n",
    "                # for polite: the prompt is \"can you make the following message more {direct}? {polite message}\"\n",
    "                # for direct: the prompt is \"can you make the following message more {polite}? {direct message}\"\n",
    "                self.messages.append({\"role\": \"user\", \"content\": f\"Can you make the following message more {after}? {data[before]}\"})\n",
    "                self.messages.append({\"role\": \"assistant\", \"content\": f\"{data[after]}\"})\n",
    "\n",
    "    def say(self, prompt: str, directify: bool, few_shot: True):\n",
    "        \n",
    "        \"\"\"Prompts the model for a response. This adds the last user prompt to the messages and prompts the model.\"\"\"\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "        if few_shot:\n",
    "            self.few_shot(directify=directify)\n",
    "        \n",
    "        if directify:\n",
    "            base = \"Please rephrase this message to be more direct semi-formal work email, like how a steoreotypical white man with priviledges would write it. Make it more concise.\"\n",
    "        else:\n",
    "            base = \"Please make this message more polite, warm and enthusiastic, while still keeping the tone semi-formal.\\n\"\n",
    "\n",
    "        self.messages.append({ \"role\": \"user\", \"content\": base + prompt})\n",
    "        response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=self.messages, temperature=self.temperature)\n",
    "        return response.choices[0][\"message\"][\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Dear James,\\n\\nI'm Raisa, co-head of the web and software team for NU Sci magazine. I heard you're the president of Kaleidoscope and I'd like to know more about club funding. Do you have a website I can check out? Thank you and have a great night!\\n\\nBest regards,\\nRaisa\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt = \"You are a translator helping people write better emails.\"\n",
    "model = ChatModel(system=system_prompt, dir=\"examples\", temperature=0.2)\n",
    "model.say(\"\"\"Hi James, \n",
    "My name is Raisa. I am the co-head of the web and software team for the NU Sci magazine. I'm emailing you because I heard from a friend that you're the president of Kaleidoscope and I wanted to learn more about what Kaleidoscope offers for club funding. I was wondering if you had a website or something I could look at by any chance. Thank you so much and have a great rest of your night!\"\"\", True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi there, \\nCould you please send me the files I requested earlier? \\nThank you very much, \\nLuisa'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.say(\"\"\"Hello, can you send over the files I requested earlier? \\n Thanks, \\nLuisa\"\"\", False, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
